================================================================================
CREDIT CARD DEFAULT PREDICTION - PROJECT DOCUMENTATION
TABLE OF CONTENTS

Project Overview
Business Objective
Dataset Description
Data Pipeline
Feature Engineering
Model Development
Model Evaluation
Results & Insights
Deployment
Code Execution Guide
Project Structure
Key Takeaways

================================================================================

PROJECT OVERVIEW
================================================================================

This project builds a predictive machine learning model to identify credit card
customers at risk of defaulting on their payments. It uses advanced techniques
including SMOTE (Synthetic Minority Over-sampling Technique) for handling class
imbalance and compares three different algorithms to find the best performer.
KEY TECHNOLOGIES:

Python 3.x
Scikit-learn, XGBoost, Random Forest
Pandas, NumPy for data manipulation
Matplotlib, Seaborn for visualization
Imbalanced-learn (SMOTE)
Streamlit for deployment

================================================================================
2. BUSINESS OBJECTIVE
Banks and credit institutions need to identify high-risk customers before they
default. This model:
‚úÖ PREDICTS DEFAULT RISK - Identifies customers likely to default in next month
‚úÖ ENABLES PROACTIVE INTERVENTION - Allows banks to take preventive measures
‚úÖ REDUCES FINANCIAL LOSSES - Minimizes bad debt exposure
‚úÖ IMPROVES CREDIT DECISIONS - Supports better loan approval/denial decisions
‚úÖ PROVIDES EXPLAINABILITY - Shows which factors contribute to default risk
BUSINESS IMPACT:

79.4% reduction in default rates when approving top 10% safest customers
Default rate reduced from 21.88% (overall) to 4.50% (approved customers)
Enables data-driven credit decisions saving millions in bad debt

================================================================================
3. DATASET DESCRIPTION
DATA SOURCE:
File: default of credit card clients.xls
Records: 30,000 credit card customers from Taiwan (April - September 2005)
Target Variable: default payment next month (Binary: 0 = No Default, 1 = Default)
CLASS DISTRIBUTION:

Non-defaulters (0): ~78% (23,364 customers)
Defaulters (1): ~22% (6,636 customers)
Note: Imbalanced dataset addressed using SMOTE

KEY FEATURES INCLUDED:
Feature CategoryColumn NamesCountDemographicsage, gender, education, marital_status4Credit Infocredit_limit1Repayment Statusrepay_status_sep, aug, jul, jun, may, apr6Bill Amountsbill_amt_sep, aug, jul, jun, may, apr6Payment Amountspay_amt_sep, aug, jul, jun, may, apr6Target Variabledefault payment next month1
                      |                                                |
TOTAL FEATURES:           |                                                | 24
FEATURE VALUE RANGES:

credit_limit: 10,000 - 1,000,000 NT$ (New Taiwan Dollar)
age: 21 - 79 years
gender: 1 = Male, 2 = Female
education: 1 = Graduate, 2 = University, 3 = High School, 4 = Others
marital_status: 1 = Married, 2 = Single, 3 = Others
repay_status: -2 to 8 (months of delay; -1 = pay duly, 0 = on time, 1-8 = months late)

================================================================================
4. DATA PIPELINE
STEP 1: LOAD DATA
pythonimport pandas as pd
df = pd.read_excel("default of credit card clients.xls")
df.head(5)

Reads the Excel file containing 30,000 customer records
Preserves all original 24 features for preprocessing
Initial data shape: (30000, 25) including target variable

STEP 2: EXPLORATORY DATA ANALYSIS (EDA)
Average Credit Limit Analysis
By Education Level:
pythonavg_credit = df.groupby('education')['credit_limit'].mean()
plt.bar(avg_credit.index, avg_credit.values)
plt.title("Average Credit Limit by Education")
Insight: Higher education typically correlates with higher credit limits
By Gender:
pythonavg_credit = df.groupby('gender')['credit_limit'].mean()
plt.title("Average Credit Limit by Gender")
Insight: Identifies potential gender-based credit allocation patterns
By Marital Status:
pythonavg_credit = df.groupby('marital_status')['credit_limit'].mean()
plt.title("Average Credit Limit by Marital Status")
Insight: Marital status shows correlation with creditworthiness assessment
Default Distribution Analysis
Overall Default Rate:
pythondf["default payment next month"].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title("Default Status Distribution")
Result: 21.88% default rate (6,636 out of 30,000 customers)
Defaulters by Gender:
pythonavg_credit = df.groupby('gender')['default payment next month'].sum()
plt.title("Defaulters by Gender")
Insight: Reveals gender-specific default patterns for targeted interventions
STEP 3: FEATURE ENGINEERING
Two new composite feature groups are created from raw billing/payment data:
FEATURE GROUP 1: CREDIT UTILIZATION
Measures how much of available credit is being used (financial stress indicator)
Formula: (Bill Amount / Credit Limit) √ó 100
Implementation:
pythondf['utilization_sep'] = (df['bill_amt_sep'] / df['credit_limit']) * 100
df['utilization_aug'] = (df['bill_amt_aug'] / df['credit_limit']) * 100
df['utilization_jul'] = (df['bill_amt_jul'] / df['credit_limit']) * 100
df['utilization_jun'] = (df['bill_amt_jun'] / df['credit_limit']) * 100
df['utilization_may'] = (df['bill_amt_may'] / df['credit_limit']) * 100
df['utilization_apr'] = (df['bill_amt_apr'] / df['credit_limit']) * 100
Derived Features:

avg_utilization - Average utilization across 6 months
max_utilization - Worst month utilization (highest risk indicator)

Summary Statistics:
       avg_utilization  max_utilization
count     30000.000000     30000.000000
mean         37.304795        49.496489
std          35.188994        43.304617
min         -23.259000       -10.000000
25%           2.999701         7.063631
50%          28.483403        43.070230
75%          68.792911        92.324643
max         536.430833      1068.857500
Interpretation Guidelines:

0-30% = Low usage (GOOD - customer not financially stretched) ‚úÖ
30-50% = Moderate usage (OK - manageable debt level) ‚ö†Ô∏è
50-80% = High usage (WARNING - possible financial stress) ‚ö†Ô∏è
80-100% = Maxed out (BAD - very high default risk) ‚ùå
>100% = Over limit (CRITICAL - immediate attention needed) üö®

FEATURE GROUP 2: PAYMENT RATIO
Measures ability to pay bills (repayment behavior indicator)
Formula: (Amount Paid / Bill Amount)
Implementation:
pythondf['payment_ratio_sep'] = df['pay_amt_sep'] / df['bill_amt_sep']
df.loc[df['bill_amt_sep'] == 0, 'payment_ratio_sep'] = 0

df['payment_ratio_aug'] = df['pay_amt_aug'] / df['bill_amt_aug']
df.loc[df['bill_amt_aug'] == 0, 'payment_ratio_aug'] = 0

# ... repeat for all 6 months
Derived Features:

avg_payment_ratio - Average payment ratio across 6 months
min_payment_ratio - Lowest payment ratio (worst month behavior)

Summary Statistics:
       avg_payment_ratio  min_payment_ratio
count       30000.000000       30000.000000
mean           -2.148846         -20.963255
std           114.502208         664.988355
min        -13727.477930      -82150.000000
25%             0.037628           0.000000
50%             0.065150           0.000037
75%             0.609651           0.034487
max          3703.444444           1.023438
Interpretation Guidelines:

1.0+ = Paying in full or more (EXCELLENT - surplus cash flow) ‚úÖ
0.5-1.0 = Partial payments (OK but debt growing slowly) ‚ö†Ô∏è
0.1-0.5 = Minimum payments only (BAD - debt accumulating rapidly) ‚ùå
0.0 = No payment made (VERY BAD - default imminent) üö®
Negative values = Overpayments or refunds (EXCELLENT credit behavior) ‚úÖ

Sample Feature Values (First 5 Customers):
   avg_utilization  max_utilization  avg_payment_ratio  min_payment_ratio
0         6.420000        19.565000           0.037019           0.000000
1         2.371806         2.879167           0.311916           0.000000
2        18.824630        32.487778           0.115141           0.051917
3        77.111333        98.582000           0.036396           0.024345
4        36.446333        71.670000           1.246958           0.035492
STEP 4: DATA CLEANING
python# Drop intermediate monthly columns (keep only aggregated features)
columns_to_drop = ['utilization_sep', 'utilization_aug', 'utilization_jul', 
                   'utilization_jun', 'utilization_may', 'utilization_apr',
                   'payment_ratio_sep', 'payment_ratio_aug', 'payment_ratio_jul',
                   'payment_ratio_jun', 'payment_ratio_may', 'payment_ratio_apr']
df = df.drop(columns=columns_to_drop)
Result:

Final feature count: 29 features (24 original + 4 engineered + 1 target)
Reduced redundancy while preserving critical information
Model-ready dataset with optimized feature set

Why Drop Monthly Columns?

Reduces feature dimensionality (curse of dimensionality)
Prevents multicollinearity issues
Aggregated features capture trends better than individual months
Improves model training speed and generalization

================================================================================
5. FEATURE ENGINEERING RATIONALE
WHY THESE FEATURES?

Utilization Metrics - Capture financial stress levels

High utilization = stretched thin financially = higher default risk
Industry standard metric used by credit bureaus


Payment Ratios - Show repayment capability and willingness

Low ratios = only making minimum payments = debt spiral
Direct indicator of customer payment behavior


Aggregated Statistics (avg/max/min) - Capture behavioral trends

Average = general behavior pattern
Maximum = worst-case scenario (stress test)
Minimum = weakest payment behavior


Business Interpretability - Finance teams understand these metrics

Not black-box features
Directly actionable for credit officers
Align with traditional banking risk assessment



MISSING VALUE HANDLING
python# Handle division by zero scenarios
df.loc[df['bill_amt_sep'] == 0, 'payment_ratio_sep'] = 0
Strategy:

When bill = 0, payment ratio = 0 (no outstanding debt)
Prevents NaN/Inf values that would break model training
Maintains data integrity across all 30,000 records
Conservative approach: assumes neutral behavior when no debt exists

================================================================================
6. MODEL DEVELOPMENT
DATA PREPARATION WORKFLOW
STEP 1: TRAIN-TEST SPLIT
pythonfrom sklearn.model_selection import train_test_split

X = df.drop(['default payment next month', 'ID'], axis=1)
y = df['default payment next month']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
Split Configuration:

Training Set: 24,000 customers (80%)
Test Set: 6,000 customers (20%)
random_state=42: Ensures reproducibility across runs
Stratification: Not explicitly set but class distribution maintained

Original Training Distribution:
Class 0 (No Default):  18,677 customers (78.7%)
Class 1 (Default):      5,323 customers (21.3%)
STEP 2: HANDLE CLASS IMBALANCE WITH SMOTE
pythonfrom imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
Why SMOTE (Synthetic Minority Over-sampling Technique)?

Original imbalance: 78.7% vs 21.3% (3.7:1 ratio)
Creates synthetic minority samples using k-nearest neighbors
Prevents model from simply predicting "no default" for everyone
Improves minority class detection without losing majority information

CRITICAL: SMOTE applied ONLY to training data

Test set remains untouched (original 21.88% default rate)
Prevents data leakage that would inflate performance metrics
Ensures realistic evaluation on real-world distribution

SMOTE Balanced Training Distribution:
Class 0 (No Default):  18,677 customers (50.0%)
Class 1 (Default):     18,677 customers (50.0%)
Total Training:        37,354 samples
Result: Perfect 50-50 balance for model training
STEP 3: FEATURE SCALING
pythonfrom sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)
Why StandardScaler?

Normalizes features to mean=0, standard deviation=1
Prevents features with large ranges from dominating

Example: credit_limit (10,000-1,000,000) vs age (21-79)


Improves gradient descent convergence
Essential for Logistic Regression and distance-based algorithms

CRITICAL BEST PRACTICE:

Scaler fitted ONLY on training data (.fit_transform)
Test data uses training statistics (.transform only)
Prevents data leakage from test set into training process

Formula: z = (x - Œº) / œÉ

Œº = mean from training data
œÉ = standard deviation from training data

================================================================================
6A. MODEL 1: LOGISTIC REGRESSION
pythonfrom sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train_scaled, y_train_res)
pred_lr = lr.predict_proba(X_test_scaled)[:, 1]
auc_lr = roc_auc_score(y_test, pred_lr)
HYPERPARAMETERS:

max_iter=1000 - Maximum iterations for convergence
random_state=42 - Reproducibility
Default penalty: L2 regularization (prevents overfitting)

CHARACTERISTICS:

‚úÖ Simple, interpretable linear model
‚úÖ Fast training (<1 second) and prediction
‚úÖ Good baseline for comparison
‚úÖ Provides feature coefficients (weights)
‚ùå Assumes linear relationship between features and log-odds
‚ùå May miss complex non-linear patterns

PERFORMANCE RESULT:
AUC Score: 0.7023 (70.23%)
Interpretation:

Decent baseline performance
70% chance of correctly ranking a random defaulter above a non-defaulter
Establishes floor performance for comparison with ensemble methods

USE CASE:

Quick predictions when speed is critical
Scenarios requiring interpretability (regulatory compliance)
Baseline model for initial deployments

================================================================================
6B. MODEL 2: RANDOM FOREST (WINNER üèÜ)
pythonfrom sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=12,
    min_samples_leaf=50,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train_scaled, y_train_res)
pred_rf = rf.predict_proba(X_test_scaled)[:, 1]
auc_rf = roc_auc_score(y_test, pred_rf)
HYPERPARAMETERS EXPLAINED:
ParameterValuePurposen_estimators400Number of decision trees (more = better, slower)max_depth12Maximum tree depth (prevents overfitting)min_samples_leaf50Minimum samples per leaf (smooths predictions)random_state42Reproducibilityn_jobs-1Use all CPU cores (parallel processing)
HOW IT WORKS:

Builds 400 independent decision trees
Each tree trained on random subset of data (bootstrap sampling)
Each split considers random subset of features
Final prediction = average of all 400 tree predictions
Ensemble wisdom: Many weak learners ‚Üí one strong learner

CHARACTERISTICS:

‚úÖ Captures non-linear relationships and interactions
‚úÖ Robust to outliers and missing values
‚úÖ Provides feature importance rankings
‚úÖ Minimal hyperparameter tuning needed
‚úÖ Resistant to overfitting due to averaging
‚ùå Slower training than Logistic Regression
‚ùå Larger model file size (~50-100MB)

PERFORMANCE RESULT:
AUC Score: 0.7643 (76.43%) üèÜ BEST MODEL
Interpretation:

WINNER: Highest AUC among all three models
76% chance of correctly ranking defaulters
+6.2% improvement over Logistic Regression (0.7643 vs 0.7023)
Strong performance indicating good generalization

TRAINING TIME: ~15-30 seconds (400 trees with parallelization)
USE CASE:

Production deployments requiring accuracy
Scenarios with complex customer behavior patterns
When interpretability is less critical than performance
SELECTED FOR FINAL DEPLOYMENT

================================================================================
6C. MODEL 3: XGBOOST
pythonimport xgboost as xgb

xgb_model = xgb.XGBClassifier(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.03,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='auc',
    n_jobs=-1,
    verbosity=0
)
xgb_model.fit(X_train_scaled, y_train_res)
pred_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]
auc_xgb = roc_auc_score(y_test, pred_xgb)
HYPERPARAMETERS EXPLAINED:
ParameterValuePurposen_estimators500Number of boosting roundsmax_depth6Maximum tree depth (shallow trees prevent overfit)learning_rate0.03Conservative learning (0.03 = 3% per round)subsample0.8Use 80% of training data per tree (prevents overfit)colsample_bytree0.8Use 80% of features per tree (regularization)eval_metricaucOptimize for AUC score during trainingn_jobs-1Use all CPU coresverbosity0Silent training (no console output)
HOW IT WORKS:

Builds trees sequentially (not in parallel like Random Forest)
Each new tree corrects errors made by previous trees
Gradient boosting: optimizes loss function via gradient descent
Adds trees weighted by learning_rate (conservative = 0.03)
Combines 500 weak learners into powerful ensemble

CHARACTERISTICS:

‚úÖ State-of-the-art gradient boosting algorithm
‚úÖ Handles complex non-linear patterns excellently
‚úÖ Built-in regularization (L1, L2, dropout)
‚úÖ Fast training with tree-specific optimizations
‚úÖ Excellent feature importance via gain/cover/frequency
‚ùå More hyperparameters to tune than Random Forest
‚ùå Slightly more prone to overfitting without careful tuning

PERFORMANCE RESULT:
AUC Score: 0.7549 (75.49%)
Interpretation:

Strong performance, competitive with Random Forest
75.5% ranking accuracy
Slightly behind Random Forest (0.7549 vs 0.7643)
Still excellent model, could potentially beat RF with more tuning

TRAINING TIME: ~20-40 seconds (500 sequential rounds)
USE CASE:

Enterprise-grade deployments
When maximum accuracy is needed
Kaggle competitions (often wins)
Large-scale production systems
Alternative to Random Forest depending on further tuning

================================================================================
7. MODEL EVALUATION - COMPREHENSIVE METRICS
MODEL COMPARISON SUMMARY
============================================================
Model                     AUC Score
============================================================
Logistic Regression       0.7023
Random Forest             0.7643 üèÜ WINNER
XGBoost                   0.7549
============================================================
Winner: Random Forest with AUC = 0.7643
Performance Gap Analysis:

Random Forest beats Logistic Regression by +6.2%
Random Forest beats XGBoost by +0.94%
Both ensemble methods significantly outperform linear baseline

METRIC 1: AUC-ROC SCORE (PRIMARY METRIC)
Definition: Area Under Receiver Operating Characteristic Curve
Range: 0.5 (random guessing) to 1.0 (perfect classification)
Our Score: 0.7643 (Random Forest)
Interpretation Scale:
0.5         = Random guessing ‚ùå
0.5-0.6     = Poor ‚ùå
0.6-0.7     = Fair ‚ö†Ô∏è
0.7-0.8     = Good ‚úÖ
0.8-0.9     = Excellent ‚úÖ‚úÖ
0.9-1.0     = Outstanding ‚úÖ‚úÖ‚úÖ
What it Measures:

Probability that model ranks a random defaulter higher than a random non-defaulter
Our model: 76.43% chance of correct ranking
Threshold-independent metric (evaluates all possible cutoffs)

Business Interpretation:
If we randomly pick one defaulter and one non-defaulter, our model will correctly
identify the defaulter 76.43% of the time.
METRIC 2: GINI COEFFICIENT (BANKING STANDARD)
pythonauc = roc_auc_score(y_test, best_pred_proba)
gini = 2 * auc - 1
Formula: Gini = 2 √ó AUC - 1
Our Result:
AUC:   0.7643
Gini:  0.5285 (52.85%)
Why Banks Use Gini:

Industry standard metric for credit risk models
Easier interpretation than AUC (0-100% scale)
Directly comparable across different models and institutions
Required for Basel II/III regulatory compliance

Interpretation Scale:
Gini < 0.20   = Poor model ‚ùå
Gini 0.20-0.40 = Fair model ‚ö†Ô∏è
Gini 0.40-0.60 = Good model ‚úÖ
Gini > 0.60   = Excellent model ‚úÖ‚úÖ
Our Assessment: 0.5285 = GOOD credit risk model
Banking Context:
Most commercial credit scorecards have Gini coefficients between 0.40-0.60.
Our model falls solidly in the acceptable range for deployment.
METRIC 3: KOLMOGOROV-SMIRNOV (KS) STATISTIC
pythonfrom scipy.stats import ks_2samp

ks_stat, p_value = ks_2samp(
    best_pred_proba[y_test == 0],   # Scores of good customers
    best_pred_proba[y_test == 1]    # Scores of bad customers
)
Our Results:
KS Statistic: 0.4103 (41.03%)
KS p-value:   1.49e-155 (highly significant)
What KS Measures:

Maximum separation between score distributions of good vs bad customers
Point of greatest divergence between cumulative distributions
Higher = better discrimination

Interpretation Scale:
KS < 0.20    = Poor separation ‚ùå
KS 0.20-0.30 = Fair separation ‚ö†Ô∏è
KS 0.30-0.40 = Good separation ‚úÖ
KS > 0.40    = Excellent separation ‚úÖ‚úÖ
Our Assessment: 0.4103 = EXCELLENT separation
Statistical Significance:

p-value = 1.49e-155 (essentially zero)
Conclusive evidence that our model distinguishes between classes
Result is not due to chance

Visual Interpretation:
The KS statistic of 41% means at the optimal cutoff point, our model creates
a 41 percentage point gap between the cumulative percentages of good and bad
customers. This is considered excellent discrimination power.
================================================================================
8. BUSINESS EVALUATION: APPROVAL RATE ANALYSIS
METHODOLOGY
pythondf_scores = pd.DataFrame({
    'true': y_test.values,
    'prob_default': best_pred_proba
})

# Sort by predicted probability (lower score = lower risk)
df_scores = df_scores.sort_values('prob_default', ascending=True)
Strategy: Approve only the top X% of customers with lowest predicted default risk
SCENARIO 1: 5% APPROVAL RATE (ULTRA-CONSERVATIVE)
--- At 5% Approval Rate (Top 5% safest customers) ---
Approved customers           : 300
Default rate in approved     : 3.67%   ‚Üê Key business metric!
Overall portfolio default rate: 21.88%
Bad rate reduction           : 83.2%
Recall (bad in rejected)     : 0.8%
Interpretation:
Business Impact:

By approving only the SAFEST 5% (300 customers), default rate drops from 21.88% ‚Üí 3.67%
83.2% reduction in default rates
Only 11 out of 300 approved customers will default
Use Case: High-value credit cards, premium products, lowest-risk lending

Financial Example:
Portfolio Value: $10,000,000
Overall Loss (21.88%): $2,188,000
Approved Loss (3.67%): $367,000
SAVINGS: $1,821,000 (83.2% reduction)
Trade-offs:

‚úÖ Extremely low risk portfolio
‚úÖ Minimal defaults and losses
‚ùå Very limited customer base (only 300 approvals)
‚ùå Potentially rejecting profitable customers

SCENARIO 2: 10% APPROVAL RATE (RECOMMENDED)
--- At 10% Approval Rate (Top 10% safest customers) ---
Approved customers           : 600
Default rate in approved     : 4.50%   ‚Üê Key business metric!
Overall portfolio default rate: 21.88%
Bad rate reduction           : 79.4%
Recall (bad in rejected)     : 2.1%
Interpretation:
Business Impact:

Approving top 10% (600 customers) yields 4.50% default rate
79.4% reduction from baseline 21.88%
27 out of 600 approved customers will default
Use Case: Standard credit cards, balanced risk-reward, scalable strategy

Financial Example:
Portfolio Value: $20,000,000
Overall Loss (21.88%): $4,376,000
Approved Loss (4.50%): $900,000
SAVINGS: $3,476,000 (79.4% reduction)
Why This is Optimal:

‚úÖ Still 79%+ reduction in default rates
‚úÖ Doubles customer base vs 5% approval (600 vs 300)
‚úÖ Balances risk mitigation with business growth
‚úÖ RECOMMENDED for most banking scenarios

Comparison Table:
MetricOverall Portfolio5% Approval10% ApprovalCustomers6,000300600Default Rate21.88%3.67%4.50%Risk ReductionBaseline83.2%79.4%Defaults Expected1,3131127Good Customers Rejected05,7005,400
CONFUSION MATRIX AT 10% CUTOFF
pythoncutoff_pct = 10
approve_n = int(total_customers * cutoff_pct / 100)
threshold_score = df_scores.iloc[approve_n - 1]['prob_default']

y_pred_cutoff = (best_pred_proba >= threshold_score).astype(int)
cm = confusion_matrix(y_test, y_pred_cutoff)
Visual Confusion Matrix:
                    Predicted: Approved  |  Predicted: Rejected
                    (Low Risk)           |  (High Risk)
-----------------------------------------------------------------------
Actual: No Default  |     TN: 4,687      |     FP: 0
(Good Customer)     |                    |
-----------------------------------------------------------------------
Actual: Default     |     FN: 1,286      |     TP: 27
(Bad Customer)      |                    |
-----------------------------------------------------------------------
Matrix Interpretation:

TN (True Negative) = 4,687: Good customers correctly rejected (too risky for top 10%)
TP (True Positive) = 27: Defaulters correctly rejected
FN (False Negative) = 1,286: Defaulters incorrectly approved (our risk exposure)
FP (False Positive) = 0: Good customers incorrectly rejected (minimal at 10% cutoff)

================================================================================
10. DEPLOYMENT
================================================================================

SAVE BEST MODEL
----------------
import joblib
joblib.dump(best_model, 'best_credit_risk_model.pkl')

What's saved:
- Trained model (weights, parameters, trees)
- Can be loaded later for predictions
- Size: ~50-200MB depending on model

LOAD MODEL FOR PREDICTIONS
---------------------------
model = joblib.load('best_credit_risk_model.pkl')
predictions = model.predict_proba(new_customer_data)[:, 1]

STREAMLIT WEB APP
-----------------
See app.py for interactive web interface that:
- Accepts customer input via sidebar
- Generates real-time default risk predictions
- Displays SHAP explanations
- Shows risk score visualization

================================================================================
11. CODE EXECUTION GUIDE
================================================================================

INSTALLATION
------------
1. Install Python 3.8+

2. Install dependencies:
   pip install -r requirements.txt

RUNNING THE FULL PIPELINE
--------------------------
python fraud_detection_full.py

Output:
- Preprocessed dataset with new features
- EDA plots (utilization, education, gender, marital status)
- Model training results
- Performance comparison (AUC scores)
- Gini/KS statistics
- Confusion matrix visualization
- Saved model file

RUNNING THE STREAMLIT APP
--------------------------
streamlit run app.py

Then open http://localhost:8501 in your browser

================================================================================
12. PROJECT STRUCTURE
================================================================================

Credit Card Default Prediction/
‚îú‚îÄ‚îÄ default of credit card clients.xls    # Raw data
‚îú‚îÄ‚îÄ credit_risk.ipynb                 # Jupyter notebook
‚îú‚îÄ‚îÄ app.py                                # Streamlit web app
‚îú‚îÄ‚îÄ requirements.txt                      # Dependencies
‚îú‚îÄ‚îÄ best_credit_risk_model.pkl            # Saved trained model
‚îî‚îÄ‚îÄ README.md                             # Project documentation

================================================================================
13. KEY TAKEAWAYS
================================================================================

‚úÖ PROBLEM: Identifying credit card default risk
‚úÖ SOLUTION: ML model using SMOTE + 3 algorithms
‚úÖ BEST MODEL: XGBoost (highest AUC)
‚úÖ BUSINESS IMPACT: 77% reduction in default rates at 10% approval
‚úÖ DEPLOYMENT: Ready for production via Streamlit or API
‚úÖ EXPLAINABILITY: SHAP provides customer-level insights

================================================================================
14. CONTACT & SUPPORT
================================================================================

For questions or improvements:
1. Review the Jupyter notebook for detailed analysis
2. Check requirements.txt for dependency versions
3. Examine app.py for deployment implementation
4. Refer to scikit-learn documentation for algorithm details

================================================================================
END OF DOCUMENTATION
================================================================================